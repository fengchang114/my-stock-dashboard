import streamlit as st
import requests
import pandas as pd
import datetime
import urllib3
import io

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
st.set_page_config(page_title="æ³•äººè²·è³£è¶…æ’è¡Œ", layout="wide")

INDUSTRY_CODE_MAP = {
    "01": "æ°´æ³¥å·¥æ¥­", "02": "é£Ÿå“å·¥æ¥­", "03": "å¡‘è† å·¥æ¥­", "04": "ç´¡ç¹”çº–ç¶­",
    "05": "é›»æ©Ÿæ©Ÿæ¢°", "06": "é›»å™¨é›»çºœ", "07": "åŒ–å­¸ç”ŸæŠ€é†«ç™‚", "08": "ç»ç’ƒé™¶ç“·",
    "09": "é€ ç´™å·¥æ¥­", "10": "é‹¼éµå·¥æ¥­", "11": "æ©¡è† å·¥æ¥­", "12": "æ±½è»Šå·¥æ¥­",
    "13": "é›»å­å·¥æ¥­", "14": "å»ºæç‡Ÿé€ ", "15": "èˆªé‹æ¥­", "16": "è§€å…‰é¤æ—…",
    "17": "é‡‘èä¿éšª", "18": "è²¿æ˜“ç™¾è²¨", "19": "ç¶œåˆ", "20": "å…¶ä»–",
    "21": "åŒ–å­¸å·¥æ¥­", "22": "ç”ŸæŠ€é†«ç™‚", "23": "æ²¹é›»ç‡ƒæ°£", "24": "åŠå°é«”æ¥­",
    "25": "é›»è…¦åŠé€±é‚Š", "26": "å…‰é›»æ¥­", "27": "é€šä¿¡ç¶²è·¯", "28": "é›»å­é›¶çµ„ä»¶",
    "29": "é›»å­é€šè·¯", "30": "è³‡è¨Šæœå‹™", "31": "å…¶ä»–é›»å­", "32": "æ–‡åŒ–å‰µæ„",
    "33": "è¾²æ¥­ç§‘æŠ€", "34": "é›»å­å•†å‹™", "35": "ç¶ èƒ½ç’°ä¿", "36": "æ•¸ä½é›²ç«¯",
    "37": "é‹å‹•ä¼‘é–’", "38": "å±…å®¶ç”Ÿæ´»", "80": "ç®¡ç†è‚¡ç¥¨", "91": "å­˜è¨—æ†‘è­‰",
}

def convert_to_float(val):
    try:
        val_str = str(val).strip()
        if val_str in ['-', '', 'nan', 'None']: return 0.0
        return float(val_str.replace(',', ''))
    except: return 0.0

@st.cache_data(ttl=86400)
def get_industry_map():
    industry_map = {}
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        res = requests.get("https://openapi.twse.com.tw/v1/opendata/t187ap03_L", headers=headers, verify=False, timeout=5)
        for row in res.json(): industry_map[row.get('å…¬å¸ä»£è™Ÿ', '').strip()] = INDUSTRY_CODE_MAP.get(row.get('ç”¢æ¥­åˆ¥', ''), 'å…¶ä»–')
        res = requests.get("https://www.tpex.org.tw/openapi/v1/t187ap03_O", headers=headers, verify=False, timeout=5)
        for row in res.json(): industry_map[row.get('å…¬å¸ä»£è™Ÿ', '').strip()] = INDUSTRY_CODE_MAP.get(row.get('ç”¢æ¥­åˆ¥', ''), 'å…¶ä»–')
    except: pass
    return industry_map

@st.cache_data(ttl=3600)
def fetch_twse_data(date_obj):
    date_str = date_obj.strftime('%Y%m%d')
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        url_chips = f"https://www.twse.com.tw/rwd/zh/fund/T86?date={date_str}&selectType=ALL&response=json"
        res = requests.get(url_chips, headers=headers, verify=False, timeout=10).json()
        if res.get('stat') != 'OK': return None
        df_chips = pd.DataFrame(res['data'], columns=res['fields']).iloc[:, [0, 1, 4, 10, 11]]
        df_chips.columns = ['ä»£è™Ÿ', 'åç¨±', 'å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†']

        url_price = f"https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX?date={date_str}&type=ALL&response=json"
        res_price = requests.get(url_price, headers=headers, verify=False, timeout=10).json()
        target_table = next((t for t in res_price.get('tables', []) if 'æ”¶ç›¤åƒ¹' in t['fields']), None)
        df_price = pd.DataFrame(target_table['data'], columns=target_table['fields'])[['è­‰åˆ¸ä»£è™Ÿ', 'æˆäº¤è‚¡æ•¸', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œ(+/-)', 'æ¼²è·Œåƒ¹å·®']]
        df_price.columns = ['ä»£è™Ÿ', 'æˆäº¤é‡_è‚¡', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œç¬¦è™Ÿ', 'æ¼²è·Œåƒ¹å·®']

        url_pe = f"https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU_d?date={date_str}&selectType=ALL&response=json"
        res_pe = requests.get(url_pe, headers=headers, verify=False, timeout=10).json()
        if res_pe.get('stat') == 'OK':
            df_pe = pd.DataFrame(res_pe['data']).iloc[:, [0, 5, 6]]
            df_pe.columns = ['ä»£è™Ÿ', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”']
        else: df_pe = pd.DataFrame(columns=['ä»£è™Ÿ', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”'])

        def calc_change(row):
            sign, val = str(row['æ¼²è·Œç¬¦è™Ÿ']).lower(), str(row['æ¼²è·Œåƒ¹å·®'])
            try:
                v = float(val.replace(',', ''))
                return v * -1 if 'green' in sign or '-' in sign else v
            except: return 0.0
        
        df_price['æ¼²è·Œ'] = df_price.apply(calc_change, axis=1)
        merged = pd.merge(df_chips, df_price[['ä»£è™Ÿ', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œ', 'æˆäº¤é‡_è‚¡']], on='ä»£è™Ÿ', how='left')
        return pd.merge(merged, df_pe, on='ä»£è™Ÿ', how='left')
    except: return None

@st.cache_data(ttl=3600)
def fetch_tpex_data(date_obj):
    roc_year = date_obj.year - 1911
    date_str = f"{roc_year}/{date_obj.strftime('%m/%d')}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        url_chips = f"https://www.tpex.org.tw/web/stock/3insti/daily_trade/3itrade_hedge_result.php?l=zh-tw&o=json&se=AL&t=D&d={date_str}"
        res = requests.get(url_chips, headers=headers, verify=False, timeout=10).json()
        raw = res.get('aaData') or (res.get('tables')[0]['data'] if res.get('tables') else [])
        if not raw: return None
        df_chips = pd.DataFrame(raw).iloc[:, [0, 1, 10, 13, 22]]
        df_chips.columns = ['ä»£è™Ÿ', 'åç¨±', 'å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†']

        url_price = f"https://www.tpex.org.tw/web/stock/aftertrading/daily_close_quotes/stk_quote_result.php?l=zh-tw&o=json&d={date_str}"
        res_price = requests.get(url_price, headers=headers, verify=False, timeout=10).json()
        df_price = pd.DataFrame(res_price['aaData']).iloc[:, [0, 2, 3, 8]]
        df_price.columns = ['ä»£è™Ÿ', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œ', 'æˆäº¤é‡_è‚¡']

        url_pe = f"https://www.tpex.org.tw/web/stock/aftertrading/peratio_analysis/pera_result.php?l=zh-tw&o=json&d={date_str}"
        res_pe = requests.get(url_pe, headers=headers, verify=False, timeout=10).json()
        raw_pe = res_pe.get('tables', [{}])[0].get('data', []) or res_pe.get('aaData', [])
        if raw_pe:
            df_pe = pd.DataFrame(raw_pe).iloc[:, [0, 2, 6]]
            df_pe.columns = ['ä»£è™Ÿ', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”']
        else: df_pe = pd.DataFrame(columns=['ä»£è™Ÿ', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”'])

        merged = pd.merge(df_chips, df_price, on='ä»£è™Ÿ', how='left')
        return pd.merge(merged, df_pe, on='ä»£è™Ÿ', how='left')
    except: return None

# ç¶²é ä»‹é¢
st.title("ğŸ“Š æ³•äººè²·è³£è¶…æ’è¡Œ (å«æ¯è‚¡æ·¨å€¼)")

with st.sidebar:
    st.header("è¨­å®š")
    target_date = st.date_input("é¸æ“‡æŸ¥è©¢æ—¥æœŸ", datetime.date.today())
    run_btn = st.button("é–‹å§‹æŠ“å–èˆ‡åˆ†æ", use_container_width=True)

if run_btn:
    with st.spinner("æ­£åœ¨ä¸‹è¼‰å…¨å¸‚å ´è³‡æ–™èˆ‡ç”¢æ¥­åœ°åœ–..."):
        industry_map = get_industry_map()
        df_twse = fetch_twse_data(target_date)
        df_tpex = fetch_tpex_data(target_date)

    if df_twse is None and df_tpex is None:
        st.error(f"{target_date} æŸ¥ç„¡è³‡æ–™æˆ–ä¼‘å¸‚ã€‚")
    else:
        df_all = pd.concat([d for d in [df_twse, df_tpex] if d is not None], ignore_index=True)
        
        # æ•¸å€¼è½‰æ›èˆ‡è™•ç†
        for col in ['å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†', 'æˆäº¤é‡_è‚¡']:
            df_all[col] = pd.to_numeric(df_all[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
        
        df_all['å¤–è³‡'] //= 1000
        df_all['æŠ•ä¿¡'] //= 1000
        df_all['è‡ªç‡Ÿå•†'] //= 1000
        df_all['æˆäº¤é‡'] = df_all['æˆäº¤é‡_è‚¡'] // 1000
        df_all['æ³•äººè²·è³£è¶…'] = df_all['å¤–è³‡'] + df_all['æŠ•ä¿¡'] + df_all['è‡ªç‡Ÿå•†']
        
        for col in ['æ”¶ç›¤åƒ¹', 'æ¼²è·Œ', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”']:
            df_all[col] = df_all[col].apply(convert_to_float)

        # è¨ˆç®—æ¯è‚¡æ·¨å€¼
        df_all['æ¯è‚¡æ·¨å€¼'] = df_all.apply(lambda r: round(r['æ”¶ç›¤åƒ¹'] / r['è‚¡åƒ¹æ·¨å€¼æ¯”'], 2) if r['è‚¡åƒ¹æ·¨å€¼æ¯”'] > 0 else 0, axis=1)

        # ç¯©é¸èˆ‡æ•´ç†
        df_all['ä»£è™Ÿ'] = df_all['ä»£è™Ÿ'].astype(str).str.strip()
        df_stock = df_all[~df_all['ä»£è™Ÿ'].str.startswith('00') & (df_all['ä»£è™Ÿ'].str.len() < 6)].copy()
        df_stock['ç”¢æ¥­é¡åˆ¥'] = df_stock['ä»£è™Ÿ'].map(industry_map).fillna('å…¶ä»–')

        output_cols = ['æ’å', 'ä»£è™Ÿ', 'åç¨±', 'ç”¢æ¥­é¡åˆ¥', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œ', 'æˆäº¤é‡', 'å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†', 'æ³•äººè²·è³£è¶…', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”', 'æ¯è‚¡æ·¨å€¼']

        def make_rank_df(df, asc):
            res = df.sort_values(by='æ³•äººè²·è³£è¶…', ascending=asc).head(100).copy().reset_index(drop=True)
            res['æ’å'] = res.index + 1
            return res[output_cols]

        df_buy = make_rank_df(df_stock, False).rename(columns={'æ³•äººè²·è³£è¶…': 'æ³•äººè²·è¶…'})
        df_sell = make_rank_df(df_stock, True).rename(columns={'æ³•äººè²·è³£è¶…': 'æ³•äººè³£è¶…'})

        tab1, tab2 = st.tabs(["ğŸš€ æ³•äººè²·è¶… Top 100", "ğŸ”» æ³•äººè³£è¶… Top 100"])
        with tab1: st.dataframe(df_buy, use_container_width=True, hide_index=True)
        with tab2: st.dataframe(df_sell, use_container_width=True, hide_index=True)

        # åŒ¯å‡º Excel
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df_buy.to_excel(writer, sheet_name='æ³•äººè²·è¶…Top100', index=False)
            df_sell.to_excel(writer, sheet_name='æ³•äººè³£è¶…Top100', index=False)
        output.seek(0)
        
        st.sidebar.success("âœ… åˆ†æå®Œæˆï¼")
        st.sidebar.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å ±è¡¨", data=output, file_name=f"{target_date}_æ³•äººè²·è³£è¶…æ’è¡Œ.xlsx", type="primary")